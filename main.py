import streamlit as st
import parser_helper
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Settings, load_index_from_storage, VectorStoreIndex, StorageContext
import os

# Caminho onde o √≠ndice ser√° salvo
INDEX_STORAGE_PATH = "./index_storage"

# Configura√ß√£o da p√°gina do Streamlit
st.set_page_config(page_title="Liccomp Chatbot", page_icon="ü¶ô", layout="centered",
                   initial_sidebar_state="auto", menu_items=None)

# T√≠tulo do chatbot
st.title("Liccomp Chatbot üí¨ü¶ô")

# Inicializa o hist√≥rico de mensagens do chat
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "Ol√°, tudo bem? Ficarei feliz em ajudar! :)"}
    ]

# Chave da API do Google para uso dos modelos da Gemini
GOOGLE_API_KEY = st.secrets["GOOGLE_API"]

# Chave API do Llama parse para converter documentos .pdf em markdown
LLAMAPARSE_API_KEY = st.secrets["LLAMAPARSE_API"]


@st.cache_resource(show_spinner=False)
def load_data():
    """
    Carrega o √≠ndice salvo do armazenamento ou cria um novo se ele n√£o existir.
    """

    # Configura√ß√£o do parser para dividir o texto em senten√ßas menores
    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)

    # Configura√ß√£o do modelo de embeddings e LLM da Gemini
    Settings.embed_model = GeminiEmbedding(
        model_name="models/gemini-embedding-exp-03-07", api_key=GOOGLE_API_KEY
    )
    llm = Gemini(api_key=GOOGLE_API_KEY, model_name="models/gemini-1.5-flash", temperature=0.2)

    # Configura√ß√£o dos modelos globais no Settings
    Settings.llm = llm
    Settings.node_parser = node_parser

    if os.path.exists(INDEX_STORAGE_PATH):
        with st.spinner("Carregando √≠ndice salvo..."):
            storage_context = StorageContext.from_defaults(persist_dir=INDEX_STORAGE_PATH)
            return load_index_from_storage(storage_context)

    with st.spinner(text="Carregando e indexando os documentos. Isso pode levar 1-2 minutos..."):
        # Leitura do arquivo PDF com o parser helper
        docs = parser_helper.le_arquivo(LLAMAPARSE_API_KEY)

        # Cria√ß√£o do √≠ndice vetorial a partir dos documentos
        index = VectorStoreIndex.from_documents(docs, transformation=[node_parser, llm])

        # Salva o √≠ndice para futuras execu√ß√µes
        index.storage_context.persist(persist_dir=INDEX_STORAGE_PATH)

        return index


# Carrega o √≠ndice dos documentos
index = load_data()

# Inicializa o mecanismo de chat e de consulta se ainda n√£o estiver configurado
if "chat_engine" not in st.session_state.keys():
    st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)
    #st.session_state.query_engine = index.as_query_engine(similarity_top_k=5, response_mode="refine", verbose=True)

# Captura a entrada do usu√°rio no chat
if prompt := st.chat_input("Digite sua pergunta"):
    st.session_state.messages.append({"role": "user", "content": prompt})

# Exibe as mensagens anteriores do chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Se a √∫ltima mensagem n√£o for do assistente, gera uma nova resposta
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            print(index)  # Exibe o √≠ndice no console para debug

            # Contextualiza√ß√£o da pergunta para o assistente
            preparacao = ('Considerando que voc√™ √© um assistente prestativo, que tem como objetivo responder '
                          'perguntas relacionadas ao curso de Licenciatura em Computa√ß√£o da Universidade Federal do Paran√° '
                          '(UFPR), localizado no Setor Palotina, e que seu nome √© Jonas, responda √†s perguntas em portugu√™s (pt-br).\n\n'
                          '# Pergunta:\n\n')

            response = st.session_state.chat_engine.chat(preparacao + prompt)
            st.write(response.response)  # Exibe a resposta no chat

            # Adiciona a resposta ao hist√≥rico do chat
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message)
